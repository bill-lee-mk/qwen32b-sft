# -*- coding: utf-8 -*-


#!/usr/bin/env python3
"""
éªŒè¯Flash Attention 3æ˜¯å¦ç”Ÿæ•ˆ
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time

def verify_flash_attention():
    print("éªŒè¯Flash Attention 3...")
    
    # æµ‹è¯•æ¨¡å‹
    model_name = "Qwen/Qwen2.5-7B-Instruct"  # ç”¨å°æ¨¡å‹æµ‹è¯•
    
    print(f"åŠ è½½æ¨¡å‹: {model_name}")
    
    # æ–¹æ³•1ï¼šä½¿ç”¨Flash Attention
    print("\næ–¹æ³•1: ä½¿ç”¨Flash Attention")
    try:
        model_fa = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            attn_implementation="flash_attention_2",  # Flash Attention 3ä½¿ç”¨æ­¤æ ‡ç­¾
        )
        print("âœ… Flash Attentionæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•æ¨ç†é€Ÿåº¦
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        inputs = tokenizer("Hello, how are you?", return_tensors="pt").to(model_fa.device)
        
        start_time = time.time()
        with torch.no_grad():
            outputs = model_fa.generate(**inputs, max_new_tokens=10)
        fa_time = time.time() - start_time
        
        print(f"Flash Attentionæ¨ç†æ—¶é—´: {fa_time:.4f}ç§’")
        
    except Exception as e:
        print(f"âŒ Flash AttentionåŠ è½½å¤±è´¥: {e}")
    
    # æ–¹æ³•2ï¼šä¸ä½¿ç”¨Flash Attentionï¼ˆå¯¹æ¯”ï¼‰
    print("\næ–¹æ³•2: ä¸ä½¿ç”¨Flash Attention")
    try:
        model_normal = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            attn_implementation="eager",  # æ ‡å‡†æ³¨æ„åŠ›
        )
        print("âœ… æ ‡å‡†æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        inputs = tokenizer("Hello, how are you?", return_tensors="pt").to(model_normal.device)
        
        start_time = time.time()
        with torch.no_grad():
            outputs = model_normal.generate(**inputs, max_new_tokens=10)
        normal_time = time.time() - start_time
        
        print(f"æ ‡å‡†æ³¨æ„åŠ›æ¨ç†æ—¶é—´: {normal_time:.4f}ç§’")
        
        if 'fa_time' in locals():
            speedup = normal_time / fa_time
            print(f"\nğŸš€ Flash AttentionåŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    except Exception as e:
        print(f"âŒ æ ‡å‡†æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    # æ£€æŸ¥Flash Attentionç¼–è¯‘è·¯å¾„
    print("\n=== Flash Attentionç¼–è¯‘ä¿¡æ¯ ===")
    try:
        import flash_attn
        print(f"Flash Attentionç‰ˆæœ¬: {flash_attn.__version__}")
        print(f"å®‰è£…è·¯å¾„: {flash_attn.__file__}")
        
        # æ£€æŸ¥ç¼–è¯‘çš„æ¶æ„
        import subprocess
        result = subprocess.run(["python", "-c", "import flash_attn; print(flash_attn.__version__)"], 
                              capture_output=True, text=True)
        print(f"ç¼–è¯‘ä¿¡æ¯: {result.stdout}")
        
    except ImportError:
        print("âŒ Flash Attentionæœªå®‰è£…")

def main():
    verify_flash_attention()

if __name__ == "__main__":
    main()
